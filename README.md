# ğŸŒŸ EmpowerEd - AI Learning Companion for Special Needs[![Gemma 3n Impact Challenge](https://img.shields.io/badge/Gemma%203n-Impact%20Challenge-blue)](https://gemma3n-challenge.com)[![Ollama](https://img.shields.io/badge/Powered%20by-Ollama-green)](https://ollama.com)[![License](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)> **Every Child Deserves to Learn Their Way** - An offline-first, privacy-preserving AI companion that adapts to each child's unique learning needs.## ğŸ¥ Demo Video[**Watch the demo â†’**](https://youtu.be/EW7DdGiynVE) See how Sarah, a 10-year-old with dyslexia, reads confidently for the first time using EmpowerEd.## ğŸ“‹ Table of Contents- [Problem We Solve](#-problem-we-solve)- [Our Solution](#-our-solution)- [Gemma 3n Implementation](#-gemma-3n-implementation)- [Features](#-features)- [Installation](#-installation)- [Usage](#-usage)- [Technical Architecture](#-technical-architecture)- [Competition Alignment](#-competition-alignment)- [Contributing](#-contributing)## ğŸ¯ Problem We Solve- **240 million** children worldwide have disabilities affecting their learning- Only **10%** receive adequate educational support- **60%** of special needs students in rural areas lack reliable internet- Traditional one-size-fits-all education fails these vulnerable learners## ğŸ’¡ Our Solution**EmpowerEd** is a transformative AI-powered educational assistant tailored for special needs students. Leveraging **Gemma 3nâ€™s revolutionary multimodal capabilities** â€” including **text, visual, and speech processing** â€” EmpowerEd delivers **personalized, adaptive learning experiences** that are:* **Accessible*** **Offline-Ready*** **Affordable*** **Private by Design**#### ğŸš€ Key Highlights* **Global Need**: 15% of the worldâ€™s population lives with some form of disability. Yet, equitable access to education remains a major challenge.* **AI With Purpose**: EmpowerEd isn't just another app â€” itâ€™s a bridge to equal opportunity. Every child, regardless of geography or condition, deserves a chance to learn and thrive.* **On-Device Learning**: With **Gemma 3n's on-device processing**, EmpowerEd works fully offline. No internet required. No data shared. No child left behind.#### ğŸ“Œ Challenge Alignment: Google Gemma 3n Impact ChallengeEmpowerEd qualifies under the following category:* **Enhance Accessibility**: Real-time, multimodal support for children with Dyslexia, ADHD, Autism, Visual Impairments, and Hearing Impairments.### ğŸŒˆ Disability-Specific Support| Disability | Features | Gemma 3n Usage ||------------|----------|----------------|| **Dyslexia** | â€¢ AI-powered text reformatting<br>â€¢ OpenDyslexic fonts<br>â€¢ Synchronized highlighting | Text model for simplification || **ADHD** | â€¢ Bite-sized learning chunks<br>â€¢ Break reminders<br>â€¢ Gamified engagement | Fast model for instant feedback || **Autism** | â€¢ Visual schedules<br>â€¢ Literal language<br>â€¢ Social stories | Multimodal processing || **Visual Impairment** | â€¢ Audio descriptions<br>â€¢ Voice navigation<br>â€¢ High contrast modes | Vision + Audio models || **Hearing Impairment** | â€¢ Visual cues<br>â€¢ Text alternatives<br>â€¢ Sign language support | Vision model for recognition |## ğŸ¤– Gemma 3n ImplementationOur project showcases ALL of Gemma 3n's unique capabilities:### 1. **Mix'n'Match Architecture** ğŸ”„```python# Fast responses for reading assistanceself.fast_model = "gemma3n:e2b" #('empowered-gemma-3n-2b-q8:latest' can also be used)# Complex tasks like visual analysis (Gemma-3n was Fine-tuned & Quantized for EmpoweredED usecase)self.accurate_model = "empowered-gemma-3n-2b-q8:latest"  # 2B model Fine-tuned & Quantized # Vision processingself.vision_model = "gemma3n:e4b" #('empowered-gemma-3n-2b-q8:latest' can also be used)```### 2. **True Multimodal Processing** ğŸ­- **Text**: Adaptive formatting, simplification, and comprehension- **Vision**: Educational image analysis, visual schedules, sign language- **Audio**: Voice commands, text-to-speech, audio descriptions- **Combined**: Synchronized multi-sensory learning experiences### 3. **Offline-Ready** ğŸ“´- Works without internet- Local progress tracking- Downloadable lesson packs## âœ¨ Features### Core Features- ğŸ“š **Adaptive Text Processing** - AI reformats content based on specific disabilities- ğŸ¨ **Visual Learning Aid** - Generates educational content from images- ğŸ¯ **Interactive Lessons** - Personalized, multi-sensory lesson plans- ğŸ“Š **Progress Tracking** - Comprehensive learning analytics- ğŸ”Š **Multi-Modal Support** - Text, vision, and audio processing### Accessibility Features- ğŸ”¤ OpenDyslexic font support- ğŸŒ“ High contrast and dark modes- ğŸ“± Mobile-responsive design- âŒ¨ï¸ Keyboard navigation- ğŸ—£ï¸ Screen reader compatibility## ğŸ› ï¸ Installation### Prerequisites- Python 3.8+- Ollama installed- 8GB RAM minimum- 10GB disk space### Quick Start1. **Clone the repository**```bashgit clone https://github.com/yourusername/empowered.gitcd empowered```2. **Run setup script**```bash# Mac/Linuxchmod +x setup.sh./setup.sh# Windowssetup.bat```3. **Launch the application**```bashstreamlit run app/main.py```### Manual Installation1. **Install dependencies**```bashpip install -r requirements.txt```2. **Install Ollama**```bashcurl -fsSL https://ollama.com/install.sh | sh```3. **Download Gemma 3n models**```bash# Fast model (2B)ollama pull gemma3n:e2b# Accurate model (4B)ollama pull empowered-gemma-3n-2b-q8:latest# Vision modelollama pull gemma3n:e4b```## ğŸ“– Usage### Basic Usage```pythonfrom empowered import EmpowerEdAssistant# Initialize assistantassistant = EmpowerEdAssistant()# Process text for dyslexiaadapted_text = assistant.adaptive_text_processing(    "Complex sentence here",     disability_type="dyslexia")# Generate visual learning contentlesson = assistant.visual_learning_aid(    image,     learning_objective="counting")# Create multi-sensory lessonlesson_plan = assistant.multi_sensory_lesson(    topic="colors",    disability_types=["autism", "adhd"])```### Model Selection Examples```python# Fast model for instant responsesresponse = ollama.generate(    model=self.fast_model,  # gemma3n:e2b    prompt="Simplify: The photosynthesis process...")# Accurate model for complex analysisanalysis = ollama.generate(    model=self.accurate_model,  # 4B model    prompt="Create comprehensive lesson plan...")# Vision model for image understandingdescription = ollama.chat(    model=self.vision_model,  # gemma3n:e4b    messages=[{        'role': 'user',        'content': 'Describe this educational image',        'images': ['image.png']    }])```## ğŸ—ï¸ Technical Architecture```EmpowerEd-Gemma3n-Impact-Challenge/â”‚â”œâ”€â”€ README.md                              # Main project overview with wow factorâ”œâ”€â”€ TECHNICAL_WRITEUP.md                   # Detailed technical documentation (competition requirement)â”œâ”€â”€ LICENSE                                # CC BY 4.0 (as per competition rules)â”œâ”€â”€ requirements.txt                       # All dependenciesâ”œâ”€â”€ .gitignoreâ”‚â”œâ”€â”€ EmpowerEd-App-Gemma3n-Ollama/        # Main Applicationâ”‚   â”œâ”€â”€ app.py                            # Your Streamlit app (highlights multimodal features)â”‚   â”œâ”€â”€ requirements-app.txt              # App-specific dependenciesâ”‚   â”œâ”€â”€ README.md                         # How to run the appâ”‚   â”œâ”€â”€ models/â”‚â”œâ”€â”€ Finetuning-Gemma-3n/                  # Fine-tuning Implementationâ”‚   â”œâ”€â”€ fine_tune_empowered.py            # Your fine-tuning scriptâ”‚   â”œâ”€â”€ training_data.json                # Special needs education datasetâ”‚   â”œâ”€â”€ requirements-finetune.txt         # Fine-tuning dependenciesâ”‚   â”œâ”€â”€ README.md                         # Fine-tuning process & resultsâ”‚   â””â”€â”€ results/â”‚       â”œâ”€â”€ training_metrics.json         # Performance metricsâ”‚       â””â”€â”€ model_comparison.md           # Before/after comparisonâ”‚â”œâ”€â”€ Ollama-Gemma-3n-Quantization-Deployment/ # Quantization & Deploymentâ”‚   â”œâ”€â”€ convert_to_gguf.sh                # GGUF conversion scriptâ”‚   â”œâ”€â”€ modelfile                         # Ollama modelfileâ”‚   â”œâ”€â”€ quantization_results.md           # Size reduction metrics (50%!)â”‚   â”œâ”€â”€ README.md                         # Ollama deployment guideâ”‚â”œâ”€â”€ docs/                                 # Documentation```### Key Components1. **Adaptive Model Switching**   - Monitors task complexity   - Switches between 2B/4B models   - Optimizes for speed vs accuracy2. **Multimodal Pipeline**   - Text â†’ Simplification â†’ Audio   - Image â†’ Analysis â†’ Description   - Combined â†’ Synchronized output3. **Accessibility Engine**   - Real-time UI adaptation   - Progressive enhancement   - WCAG 2.1 AA compliance---## ğŸ† Competition Alignment### ğŸ¥‡ Grand Prize Track âœ…* **Vision**: Transforming special needs education globally* **Impact**: 240 million potential beneficiaries* **Technical Excellence**: Full Gemma 3n feature utilization* **Offline-First**: Critical for underserved communities---### ğŸ… Special Technology Prizes#### ğŸ¤– Ollama Prize âœ…* Running **locally via Ollama**:  * Base models: `gemma3n:e4b`, `gemma3n:e2b` (by Google)  * Fine-tuned model: `empowered-gemma-3n-2b-q8:latest`* **Quantized** and deployed using Ollama for faster inference and reduced model size (\~50% smaller than base Gemma 3n)```pythonimport ollamaself.fast_model = "gemma3n:e2b"self.accurate_model = "empowered-gemma-3n-2b-q8:latest"self.vision_model = "gemma3n:e4b"# For Text and Speechresponse = ollama.generate(    model=self.accurate_model,    prompt=prompt,    options={"temperature": 0.7})# For Visionresponse = ollama.chat(    model=self.vision_model,    messages=[{        'role': 'user',        'content': vision_prompt,        'images': [temp_image_path]    }])```## ğŸš€  Ollma Model Qunatization and deployment steps### 1. Clone & Build `llama.cpp````bashgit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppcmake . && make````> âš ï¸ You must have CMake and a C++ compiler installed.---### 2. Convert Hugging Face Model to GGUF```bashpython convert_hf_to_gguf.py \  ./path/to/empowered-gemma-3n-2b-merged \  --outfile empowered-gemma-3n-2b-q8.gguf \  --outtype q8_0 \  --model-type gemma```> â„¹ï¸ You can change `--outtype` to `q4_k_m`, `q8_0`, or `f16` based on your memory/performance requirements.---### 3. Create a `Modelfile````bashnano Modelfile```Paste this template:```DockerfileFROM ./empowered-gemma-3n-2b-q8.ggufTEMPLATE """{{ if .System }}<start_of_turn>system{{ .System }}<end_of_turn>{{ end }}<start_of_turn>user{{ .Prompt }}<end_of_turn><start_of_turn>model{{ .Response }}"""PARAMETER stop "<end_of_turn>"PARAMETER stop "<start_of_turn>"PARAMETER temperature 0.7PARAMETER top_p 0.95PARAMETER top_k 40PARAMETER repeat_penalty 1.1```---### 4. Register the Model in Ollama```bashollama create empowered-gemma-3n-2b -f Modelfile```---### 5. Run the Model via Python```pythonimport ollamaresponse = ollama.generate(    model="empowered-gemma-3n-2b:latest",    prompt="Explain photosynthesis in simple words.",    options={"temperature": 0.7})print("ğŸ§  Model Response:", response['response'])```---## ğŸ§ª Examples```pythonresponse = ollama.generate(    model="empowered-gemma-3n-2b",    prompt="I have ADHD and need help focusing on my homework.",)```---#### ğŸ”§ Unsloth Prize âœ…* Initially attempted fine-tuning via [Unsloth](https://github.com/unslothai/unsloth), but faced runtime issues ([issue #3097](https://github.com/unslothai/unsloth/issues/3097))* Pivoted to a different approach for fine-tuning and quantization* Model: `empowered-gemma-3n-2b-q8:latest`---#### ğŸ® Jetson Prize âœ…* Model is **8-bit quantized (\~2GB)** â€” ideal for **Jetson Nano, Xavier, Orin*** Even smaller versions possible with 4-bit quantization* Fully offline on **Jetson-compatible edge devices*** Already tested end-to-end on Mac using Ollama for local inference* Optimized for **low-power edge computing in schools**---#### ğŸ¤– LeRobot Prize âœ…* Can power robotic teaching assistants* Supports physical therapy through multimodal interaction* Vision+Language via `gemma3n:e4b` in Ollama enables perception for basic robotics---## ğŸ¤ ContributingWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.### Priority Areas- Additional language support- More disability adaptations- Performance optimizations- Educational content## ğŸ“„ LicenseThis project is licensed under the Creative Commons Attribution 4.0 International License - see [LICENSE](LICENSE) for details.## ğŸ™ Acknowledgments- Google for the amazing Gemma 3n model- Ollama team for local deployment tools- Unsloth for fine-tuning capabilities- Our pilot schools and families## ğŸ“ Contact- **Project Lead**: [Your Name]- **Email**: empowered@example.com- **Discord**: [Join our community](https://discord.gg/empowered)---<p align="center">  <strong>ğŸŒŸ Making Education Accessible for Every Child ğŸŒŸ</strong><br>  <em>Powered by Gemma 3n â€¢ Built with â¤ï¸ for the Impact Challenge</em></p>