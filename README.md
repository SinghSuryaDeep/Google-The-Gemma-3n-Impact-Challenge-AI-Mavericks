# 🌟 EmpowerEd - AI Learning Companion for Special Needs[![Gemma 3n Impact Challenge](https://img.shields.io/badge/Gemma%203n-Impact%20Challenge-blue)](https://gemma3n-challenge.com)[![Ollama](https://img.shields.io/badge/Powered%20by-Ollama-green)](https://ollama.com)[![License](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)> **Every Child Deserves to Learn Their Way** - An offline-first, privacy-preserving AI companion that adapts to each child's unique learning needs.## 🎥 Demo Video[**Watch the demo here →**](https://youtu.be/EW7DdGiynVE) See how Sarah, a 10-year-old with Dyslexia and ADHD, reads confidently for the first time using EmpowerEd.## 📋 Table of Contents- [Problem We Solve](#-problem-we-solve)- [Our Solution](#-our-solution)- [Gemma 3n Implementation](#-gemma-3n-implementation)- [Features](#-features)- [Installation](#-installation)- [Usage](#-usage)- [Technical Architecture](#-technical-architecture)- [Competition Alignment](#-competition-alignment)- [Contributing](#-contributing)## 🎯 Problem We Solve- **240 million** children worldwide have disabilities affecting their learning- **Only 10%** receive adequate educational support- **Roughly 40-60%** of special needs students in rural areas lack reliable internet- Traditional one-size-fits-all education fails these vulnerable learners## 💡 Our Solution**EmpowerEd** is a transformative AI-powered educational assistant tailored for special needs students. Leveraging **Gemma 3n’s revolutionary multimodal capabilities** — including **text, visual, and speech processing** — EmpowerEd delivers **personalized, adaptive learning experiences** that are:* **Accessible*** **Offline-Ready** (It can be made available on the Play Store, App Store and other platforms for use on devices such as phones, tablets, and computers)* **Affordable*** **Private by Design**#### 🚀 Key Highlights* **Global Need**: 15% of the world’s population lives with some form of disability. Yet, equitable access to education remains a major challenge.* **AI With Purpose**: EmpowerEd isn't just another app — it’s a bridge to equal opportunity. Every child, regardless of geography or condition, deserves a chance to learn and thrive.* **On-Device Learning**: With **Gemma 3n's on-device processing**, EmpowerEd works fully offline. No internet required. No data shared. No child left behind.#### 📌 Challenge Alignment: Google Gemma 3n Impact ChallengeEmpowerEd qualifies under the following category:* **Enhance Accessibility**: Real-time, multimodal support for children with Dyslexia, ADHD, Autism, Visual Impairments, and Hearing Impairments.### 🌈 Disability-Specific Support| Disability | Features | Gemma 3n Usage ||------------|----------|----------------|| **Dyslexia** | • AI-powered text reformatting<br>• OpenDyslexic fonts<br>• Synchronized highlighting | Text model for simplification || **ADHD** | • Bite-sized learning chunks<br>• Break reminders<br>• Gamified engagement | Fast model for instant feedback || **Autism** | • Visual schedules<br>• Literal language<br>• Social stories | Multimodal processing || **Visual Impairment** | • Audio descriptions<br>• Voice navigation<br>• High contrast modes | Vision + Audio models || **Hearing Impairment** | • Visual cues<br>• Text alternatives<br>• Sign language support | Vision model for recognition |## 🤖 Gemma 3n ImplementationOur project showcases ALL of Gemma 3n's unique capabilities:### 1. **Mix'n'Match Architecture** 🔄```python# Fast responses for reading assistanceself.fast_model = "gemma3n:e2b" #('empowered-gemma-3n-2b-q8:latest' can also be used)# Complex tasks like visual analysis (Gemma-3n was Fine-tuned & Quantized for EmpoweredED usecase)self.accurate_model = "empowered-gemma-3n-2b-q8:latest"  # 2B model Fine-tuned & Quantized # Vision processingself.vision_model = "gemma3n:e4b" #('empowered-gemma-3n-2b-q8:latest' can also be used)```### 2. **True Multimodal Processing** 🎭- **Text**: Adaptive formatting, simplification, and comprehension- **Vision**: Educational image analysis, visual schedules, sign language- **Audio**: Voice commands, text-to-speech, audio descriptions- **Combined**: Synchronized multi-sensory learning experiences### 3. **Offline-Ready** 📴- Works without internet- Local progress tracking- Downloadable lesson packs## ✨ Features### Core Features- 📚 **Adaptive Text Processing** - AI reformats content based on specific disabilities- 🎨 **Visual Learning Aid** - Generates educational content from images- 🎯 **Interactive Lessons** - Personalized, multi-sensory lesson plans- 📊 **Progress Tracking** - Comprehensive learning analytics- 🔊 **Multi-Modal Support** - Text, vision, and audio processing### Accessibility Features- 🔤 OpenDyslexic font support- 🌓 High contrast and dark modes- 📱 Mobile-responsive design- ⌨️ Keyboard navigation- 🗣️ Screen reader compatibility## 🛠️ Installation### Prerequisites- Python 3.8+- Ollama installed- 8GB RAM or more- 10GB disk space### Quick Start1. **Clone the repository**```bashgit clone https://github.com/yourusername/empowered.gitcd empowered```2. **Run setup script**```bash# Mac/Linuxchmod +x setup.sh./setup.sh# Windowssetup.bat```3. **Launch the application**```bashstreamlit run app/main.py```### Manual Installation1. **Install dependencies**```bashpip install -r requirements.txt```2. **Install Ollama**```bashcurl -fsSL https://ollama.com/install.sh | sh```3. **Download Gemma 3n models**```bash# Fast model (2B)ollama pull gemma3n:e2b# Accurate model (4B)ollama pull empowered-gemma-3n-2b-q8:latest# Vision modelollama pull gemma3n:e4b```## 📖 Usage### Basic Usage```pythonfrom empowered import EmpowerEdAssistant# Initialize assistantassistant = EmpowerEdAssistant()# Process text for dyslexiaadapted_text = assistant.adaptive_text_processing(    "Complex sentence here",     disability_type="dyslexia")# Generate visual learning contentlesson = assistant.visual_learning_aid(    image,     learning_objective="counting")# Create multi-sensory lessonlesson_plan = assistant.multi_sensory_lesson(    topic="colors",    disability_types=["autism", "adhd"])```### Model Selection Examples```python# Fast model for instant responsesresponse = ollama.generate(    model=self.fast_model,  # gemma3n:e2b    prompt="Simplify: The photosynthesis process...")# Accurate model for complex analysisanalysis = ollama.generate(    model=self.accurate_model,  # 2B model    prompt="Create comprehensive lesson plan...")# Vision model for image understandingdescription = ollama.chat(    model=self.vision_model,  # gemma3n:e4b    messages=[{        'role': 'user',        'content': 'Describe this educational image',        'images': ['image.png']    }])```## 🏗️ Technical Architecture```EmpowerEd-Gemma3n-Impact-Challenge/│├── README.md                              # Main project overview with wow factor├── TECHNICAL_WRITEUP.md                   # Detailed technical documentation (competition requirement)├── LICENSE                                # CC BY 4.0 (as per competition rules)├── requirements.txt                       # All dependencies├── .gitignore│├── EmpowerEd-App-Gemma3n-Ollama/        # Main Application│   ├── app.py                            # Your Streamlit app (highlights multimodal features)│   ├── requirements-app.txt              # App-specific dependencies│   ├── README.md                         # How to run the app│   ├── student_profile.json              # The system can integrate with external data sources or databases to access student profiles.│   ├──progress_tracking.json             # The system can integrate with external data sources or databases to access and monitor their progress over time.│├── Finetuning-Gemma-3n/                  # Fine-tuning Implementation│   ├── fine_tune_empowered.py            # Your fine-tuning script│   ├── training_data.json                # Special needs education dataset│   ├── requirements-finetune.txt         # Fine-tuning dependencies│   ├── README.md                         # Fine-tuning process & results│   └── results/│       ├── training_metrics.json         # Performance metrics│       └── model_comparison.md           # Before/after comparison│├── Ollama-Gemma-3n-Quantization-Deployment/ # Quantization & Deployment│   ├── convert_to_gguf.sh                # GGUF conversion script│   ├── modelfile                         # Ollama modelfile│   ├── quantization_results.md           # Size reduction metrics (50%!)│   ├── README.md                         # Ollama deployment guide│├── docs/                                 # Documentation```### Key Components1. **Adaptive Model Switching**   - Monitors task complexity   - Switches between 2B/4B models   - Optimizes for speed vs accuracy2. **Multimodal Pipeline**   - Text → Simplification → Audio   - Image → Analysis → Description   - Combined → Synchronized output3. **Accessibility Engine**   - Real-time UI adaptation   - Progressive enhancement   - WCAG 2.1 AA compliance---## 🏆 Competition Alignment### 🥇 Grand Prize Track ✅* **Vision**: Transforming special needs education globally* **Impact**: 240 million potential beneficiaries* **Technical Excellence**: Full Gemma 3n feature utilization* **Offline-First**: Critical for underserved communities---### 🏅 Special Technology Prizes#### 🤖 Ollama Prize ✅* Running **locally via Ollama**:  * Base models: `gemma3n:e4b`, `gemma3n:e2b` (by Google)  * Fine-tuned model: `empowered-gemma-3n-2b-q8:latest`* **Quantized** and deployed using Ollama for faster inference and reduced model size (\~50% smaller than base Gemma 3n)```pythonimport ollamaself.fast_model = "gemma3n:e2b"self.accurate_model = "empowered-gemma-3n-2b-q8:latest"self.vision_model = "gemma3n:e4b"# For Text and Speechresponse = ollama.generate(    model=self.accurate_model,    prompt=prompt,    options={"temperature": 0.7})# For Visionresponse = ollama.chat(    model=self.vision_model,    messages=[{        'role': 'user',        'content': vision_prompt,        'images': [temp_image_path]    }])```#### 🚀  Ollma Model Quantization and deployment steps##### 1. Clone & Build `llama.cpp````bashgit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppcmake . && make````---##### 2. Convert Hugging Face Model to GGUF```bashpython convert_hf_to_gguf.py \  ./path/to/empowered-gemma-3n-2b-merged \  --outfile empowered-gemma-3n-2b-q8.gguf \  --outtype q8_0 \  --model-type gemma```> ℹ️ You can change `--outtype` to `q4_k_m`, `q8_0`, or `f16` based on your memory/performance requirements.---##### 3. Create a `Modelfile````bashnano Modelfile```Paste this template:```DockerfileFROM ./empowered-gemma-3n-2b-q8.ggufTEMPLATE """{{ if .System }}<start_of_turn>system{{ .System }}<end_of_turn>{{ end }}<start_of_turn>user{{ .Prompt }}<end_of_turn><start_of_turn>model{{ .Response }}"""PARAMETER stop "<end_of_turn>"PARAMETER stop "<start_of_turn>"PARAMETER temperature 0.7PARAMETER top_p 0.95PARAMETER top_k 40PARAMETER repeat_penalty 1.1```---##### 4. Register the Model in Ollama```bashollama create empowered-gemma-3n-2b -f Modelfile```---##### 5. Run the Model via Python```pythonimport ollamaresponse = ollama.generate(    model="empowered-gemma-3n-2b:latest",    prompt="Explain photosynthesis in simple words.",    options={"temperature": 0.7})print("🧠 Model Response:", response['response'])```---##### 🧪 Examples```pythonresponse = ollama.generate(    model="empowered-gemma-3n-2b:latest",    prompt="I have ADHD and need help focusing on my homework.",)```---#### 🔧 Unsloth Prize ✅* Initially attempted fine-tuning via [Unsloth](https://github.com/unslothai/unsloth), but faced runtime issues ([issue #3097](https://github.com/unslothai/unsloth/issues/3097)). I tried with python 3.10, 3.11,3.12, 3.13 , Unsloth fine tuning worked with none of them.* Pivoted to a different approach for fine-tuning and quantization* Model: `empowered-gemma-3n-2b-q8:latest`---#### 🎮 Jetson Prize ✅* Model is **8-bit quantized half the size** — ideal for **Jetson Nano, Xavier, Orin*** Even smaller versions possible with 4-bit quantization* Fully offline on **Jetson-compatible edge devices*** Already tested end-to-end on laptop using Ollama for local inference* Optimized for **low-power edge computing in schools**---#### 🤖 LeRobot Prize ✅* Can power robotic teaching assistants* Supports physical therapy through multimodal interaction* Vision+Language via `gemma3n:e4b` in Ollama enables perception for basic robotics---## 🤝 ContributingWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.### Priority Areas- Additional language support- More disability adaptations- Performance optimizations- Educational content## 📄 LicenseThis project is licensed under the Creative Commons Attribution 4.0 International License - see [LICENSE](LICENSE) for details.## 🙏 Acknowledgments- Google for the amazing Gemma 3n model- Ollama team for local deployment tools- Unsloth for fine-tuning capabilities- Our pilot schools and families## 📞 Contact- **Project Lead**: [Your Name]- **Email**: empowered@example.com- **Discord**: [Join our community](https://discord.gg/empowered)---<p align="center">  <strong>🌟 Making Education Accessible for Every Child 🌟</strong><br>  <em>Powered by Gemma 3n • Built with ❤️ for the Impact Challenge</em></p>