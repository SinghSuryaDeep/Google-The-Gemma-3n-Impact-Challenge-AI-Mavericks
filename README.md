# üåü EmpowerEd - AI Learning Companion for Special Needs[![Gemma 3n Impact Challenge](https://img.shields.io/badge/Gemma%203n-Impact%20Challenge-blue)](https://gemma3n-challenge.com)[![Ollama](https://img.shields.io/badge/Powered%20by-Ollama-green)](https://ollama.com)[![License](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)> **Every Child Deserves to Learn Their Way** - An offline-first, privacy-preserving AI companion that adapts to each child's unique learning needs.## üé• Demo Video[**Watch the demo here ‚Üí**](https://youtu.be/EW7DdGiynVE) See how Sarah, a 10-year-old with Dyslexia and ADHD, reads confidently for the first time using EmpowerEd.## üìã Table of Contents- [Problem We Solve](#-problem-we-solve)- [Our Solution](#-our-solution)- [Gemma 3n Implementation](#-gemma-3n-implementation)- [Features](#-features)- [Installation](#-installation)- [Usage](#-usage)- [Technical Architecture](#-technical-architecture)- [Competition Alignment](#-competition-alignment)- [Contributing](#-contributing)## üéØ Problem We Solve- **240 million** children worldwide have disabilities affecting their learning- **Only 10%** receive adequate educational support- **Roughly 40-60%** of special needs students in rural areas lack reliable internet- Traditional one-size-fits-all education fails these vulnerable learners## üí° Our Solution**EmpowerEd** is a transformative AI-powered educational assistant tailored for special needs students. Leveraging **Gemma 3n‚Äôs revolutionary multimodal capabilities** ‚Äî including **text, visual, and speech processing** ‚Äî EmpowerEd delivers **personalized, adaptive learning experiences** that are:* **Accessible*** **Offline-Ready** (It can be made available on the Play Store, App Store and other platforms for use on devices such as phones, tablets, and computers)* **Affordable*** **Private by Design**#### üöÄ Key Highlights* **Global Need**: 15% of the world‚Äôs population lives with some form of disability. Yet, equitable access to education remains a major challenge.* **AI With Purpose**: EmpowerEd isn't just another app ‚Äî it‚Äôs a bridge to equal opportunity. Every child, regardless of geography or condition, deserves a chance to learn and thrive.* **On-Device Learning**: With **Gemma 3n's on-device processing**, EmpowerEd works fully offline. No internet required. No data shared. No child left behind.#### üìå Challenge Alignment: Google Gemma 3n Impact ChallengeEmpowerEd qualifies under the following category:* **Enhance Accessibility**: Real-time, multimodal support for children with Dyslexia, ADHD, Autism, Visual Impairments, and Hearing Impairments.### üåà Disability-Specific Support| Disability | Features | Gemma 3n Usage ||------------|----------|----------------|| **Dyslexia** | ‚Ä¢ AI-powered text reformatting<br>‚Ä¢ OpenDyslexic fonts<br>‚Ä¢ Synchronized highlighting | Text model for simplification || **ADHD** | ‚Ä¢ Bite-sized learning chunks<br>‚Ä¢ Break reminders<br>‚Ä¢ Gamified engagement | Fast model for instant feedback || **Autism** | ‚Ä¢ Visual schedules<br>‚Ä¢ Literal language<br>‚Ä¢ Social stories | Multimodal processing || **Visual Impairment** | ‚Ä¢ Audio descriptions<br>‚Ä¢ Voice navigation<br>‚Ä¢ High contrast modes | Vision + Audio models || **Hearing Impairment** | ‚Ä¢ Visual cues<br>‚Ä¢ Text alternatives<br>‚Ä¢ Sign language support | Vision model for recognition |## ü§ñ Gemma 3n ImplementationOur project showcases ALL of Gemma 3n's unique capabilities:### 1. **Mix'n'Match Architecture** üîÑ```python# Fast responses for reading assistanceself.fast_model = "gemma3n:e2b" #('empowered-gemma-3n-2b-q8:latest' can also be used)# Complex tasks like visual analysis (Gemma-3n was Fine-tuned & Quantized for EmpoweredED usecase)self.accurate_model = "empowered-gemma-3n-2b-q8:latest"  # 2B model Fine-tuned & Quantized # Vision processingself.vision_model = "gemma3n:e4b" #('empowered-gemma-3n-2b-q8:latest' can also be used)```### 2. **True Multimodal Processing** üé≠- **Text**: Adaptive formatting, simplification, and comprehension- **Vision**: Educational image analysis, visual schedules, sign language- **Audio**: Voice commands, text-to-speech, audio descriptions- **Combined**: Synchronized multi-sensory learning experiences### 3. **Offline-Ready** üì¥- Works without internet- Local progress tracking- Downloadable lesson packs## ‚ú® Features### Core Features- üìö **Adaptive Text Processing** - AI reformats content based on specific disabilities- üé® **Visual Learning Aid** - Generates educational content from images- üéØ **Interactive Lessons** - Personalized, multi-sensory lesson plans- üìä **Progress Tracking** - Comprehensive learning analytics- üîä **Multi-Modal Support** - Text, vision, and audio processing### Accessibility Features- üî§ OpenDyslexic font support- üåì High contrast and dark modes- üì± Mobile-responsive design- ‚å®Ô∏è Keyboard navigation- üó£Ô∏è Screen reader compatibility## üõ†Ô∏è Installation### Prerequisites- Python 3.8+- Ollama installed- 8GB RAM or more- 10GB disk space### Quick Start1. **Clone the repository**```bashgit clone https://github.com/yourusername/empowered.gitcd empowered```2. **Run setup script**```bash# Mac/Linuxchmod +x setup.sh./setup.sh# Windowssetup.bat```3. **Launch the application**```bashstreamlit run app/main.py```### Manual Installation1. **Install dependencies**```bashpip install -r requirements.txt```2. **Install Ollama**```bashcurl -fsSL https://ollama.com/install.sh | sh```3. **Download Gemma 3n models**```bash# Fast model (2B)ollama pull gemma3n:e2b# Accurate model (4B)ollama pull empowered-gemma-3n-2b-q8:latest# Vision modelollama pull gemma3n:e4b```## üìñ Usage### Basic Usage```pythonfrom empowered import EmpowerEdAssistant# Initialize assistantassistant = EmpowerEdAssistant()# Process text for dyslexiaadapted_text = assistant.adaptive_text_processing(    "Complex sentence here",     disability_type="dyslexia")# Generate visual learning contentlesson = assistant.visual_learning_aid(    image,     learning_objective="counting")# Create multi-sensory lessonlesson_plan = assistant.multi_sensory_lesson(    topic="colors",    disability_types=["autism", "adhd"])```### Model Selection Examples```python# Fast model for instant responsesresponse = ollama.generate(    model=self.fast_model,  # gemma3n:e2b    prompt="Simplify: The photosynthesis process...")# Accurate model for complex analysisanalysis = ollama.generate(    model=self.accurate_model,  # 2B model    prompt="Create comprehensive lesson plan...")# Vision model for image understandingdescription = ollama.chat(    model=self.vision_model,  # gemma3n:e4b    messages=[{        'role': 'user',        'content': 'Describe this educational image',        'images': ['image.png']    }])```## üèóÔ∏è Technical Architecture```EmpowerEd-Gemma3n-Impact-Challenge/‚îÇ‚îú‚îÄ‚îÄ README.md                              # Main project overview with wow factor‚îú‚îÄ‚îÄ TECHNICAL_WRITEUP.md                   # Detailed technical documentation (competition requirement)‚îú‚îÄ‚îÄ LICENSE                                # CC BY 4.0 (as per competition rules)‚îú‚îÄ‚îÄ requirements.txt                       # All dependencies‚îú‚îÄ‚îÄ .gitignore‚îÇ‚îú‚îÄ‚îÄ EmpowerEd-App-Gemma3n-Ollama/        # Main Application‚îÇ   ‚îú‚îÄ‚îÄ app.py                            # Your Streamlit app (highlights multimodal features)‚îÇ   ‚îú‚îÄ‚îÄ requirements-app.txt              # App-specific dependencies‚îÇ   ‚îú‚îÄ‚îÄ README.md                         # How to run the app‚îÇ   ‚îú‚îÄ‚îÄ student_profile.json              # The system can integrate with external data sources or databases to access student profiles.‚îÇ   ‚îú‚îÄ‚îÄprogress_tracking.json             # The system can integrate with external data sources or databases to access and monitor their progress over time.‚îÇ‚îú‚îÄ‚îÄ Finetuning-Gemma-3n/                  # Fine-tuning Implementation‚îÇ   ‚îú‚îÄ‚îÄ fine_tune_empowered.py            # Your fine-tuning script‚îÇ   ‚îú‚îÄ‚îÄ training_data.json                # Special needs education dataset‚îÇ   ‚îú‚îÄ‚îÄ requirements-finetune.txt         # Fine-tuning dependencies‚îÇ   ‚îú‚îÄ‚îÄ README.md                         # Fine-tuning process & results‚îÇ   ‚îî‚îÄ‚îÄ results/‚îÇ       ‚îú‚îÄ‚îÄ training_metrics.json         # Performance metrics‚îÇ       ‚îî‚îÄ‚îÄ model_comparison.md           # Before/after comparison‚îÇ‚îú‚îÄ‚îÄ Ollama-Gemma-3n-Quantization-Deployment/ # Quantization & Deployment‚îÇ   ‚îú‚îÄ‚îÄ convert_to_gguf.sh                # GGUF conversion script‚îÇ   ‚îú‚îÄ‚îÄ modelfile                         # Ollama modelfile‚îÇ   ‚îú‚îÄ‚îÄ quantization_results.md           # Size reduction metrics (50%!)‚îÇ   ‚îú‚îÄ‚îÄ README.md                         # Ollama deployment guide‚îÇ‚îú‚îÄ‚îÄ docs/                                 # Documentation```### Key Components1. **Adaptive Model Switching**   - Monitors task complexity   - Switches between 2B/4B models   - Optimizes for speed vs accuracy2. **Multimodal Pipeline**   - Text ‚Üí Simplification ‚Üí Audio   - Image ‚Üí Analysis ‚Üí Description   - Combined ‚Üí Synchronized output3. **Accessibility Engine**   - Real-time UI adaptation   - Progressive enhancement   - WCAG 2.1 AA compliance---## üèÜ Competition Alignment### ü•á Grand Prize Track ‚úÖ* **Vision**: Transforming special needs education globally* **Impact**: 240 million potential beneficiaries* **Technical Excellence**: Full Gemma 3n feature utilization* **Offline-First**: Critical for underserved communities---### üèÖ Special Technology Prizes#### ü§ñ Ollama Prize ‚úÖ* Running **locally via Ollama**:  * Base models: `gemma3n:e4b`, `gemma3n:e2b` (by Google)  * Fine-tuned model: `empowered-gemma-3n-2b-q8:latest`* **Quantized** and deployed using Ollama for faster inference and reduced model size (\~50% smaller than base Gemma 3n)```pythonimport ollamaself.fast_model = "gemma3n:e2b"self.accurate_model = "empowered-gemma-3n-2b-q8:latest"self.vision_model = "gemma3n:e4b"# For Text and Speechresponse = ollama.generate(    model=self.accurate_model,    prompt=prompt,    options={"temperature": 0.7})# For Visionresponse = ollama.chat(    model=self.vision_model,    messages=[{        'role': 'user',        'content': vision_prompt,        'images': [temp_image_path]    }])```#### üöÄ  Ollma Model Quantization and deployment steps##### 1. Clone & Build `llama.cpp````bashgit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppcmake . && make````---##### 2. Convert Hugging Face Model to GGUF```bashpython convert_hf_to_gguf.py \  ./path/to/empowered-gemma-3n-2b-merged \  --outfile empowered-gemma-3n-2b-q8.gguf \  --outtype q8_0 \  --model-type gemma```> ‚ÑπÔ∏è You can change `--outtype` to `q4_k_m`, `q8_0`, or `f16` based on your memory/performance requirements.---##### 3. Create a `Modelfile````bashnano Modelfile```Paste this template:```DockerfileFROM ./empowered-gemma-3n-2b-q8.ggufTEMPLATE """{{ if .System }}<start_of_turn>system{{ .System }}<end_of_turn>{{ end }}<start_of_turn>user{{ .Prompt }}<end_of_turn><start_of_turn>model{{ .Response }}"""PARAMETER stop "<end_of_turn>"PARAMETER stop "<start_of_turn>"PARAMETER temperature 0.7PARAMETER top_p 0.95PARAMETER top_k 40PARAMETER repeat_penalty 1.1```---##### 4. Register the Model in Ollama```bashollama create empowered-gemma-3n-2b -f Modelfile```---##### 5. Run the Model via Python```pythonimport ollamaresponse = ollama.generate(    model="empowered-gemma-3n-2b:latest",    prompt="Explain photosynthesis in simple words.",    options={"temperature": 0.7})print("üß† Model Response:", response['response'])```---##### üß™ Examples```pythonresponse = ollama.generate(    model="empowered-gemma-3n-2b:latest",    prompt="I have ADHD and need help focusing on my homework.",)```---#### üîß Unsloth Prize ‚úÖ* Initially attempted fine-tuning via [Unsloth](https://github.com/unslothai/unsloth), but faced runtime issues ([issue #3097](https://github.com/unslothai/unsloth/issues/3097)). I tried with python 3.10, 3.11,3.12, 3.13 , Unsloth fine tuning worked with none of them.* Pivoted to a different approach for fine-tuning and quantization* Model: `empowered-gemma-3n-2b-q8:latest`---#### üéÆ Jetson Prize ‚úÖ* Model is **8-bit quantized half the size** ‚Äî ideal for **Jetson Nano, Xavier, Orin*** Even smaller versions possible with 4-bit quantization* Fully offline on **Jetson-compatible edge devices*** Already tested end-to-end on laptop using Ollama for local inference* Optimized for **low-power edge computing in schools**---#### ü§ñ LeRobot Prize ‚úÖ* Can power robotic teaching assistants* Supports physical therapy through multimodal interaction* Vision+Language via `gemma3n:e4b` in Ollama enables perception for basic robotics---## ü§ù ContributingWe welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.### Priority Areas- Additional language support- More disability adaptations- Performance optimizations- Educational content## üìÑ LicenseThis project is licensed under the Creative Commons Attribution 4.0 International License - see [LICENSE](LICENSE) for details.## üôè Acknowledgments- Google for the amazing Gemma 3n model- Ollama team for local deployment tools- Unsloth for fine-tuning capabilities- Our pilot schools and families## üìû Contact- **Project Lead**: [Your Name]- **Email**: empowered@example.com- **Discord**: [Join our community](https://discord.gg/empowered)---<p align="center">  <strong>üåü Making Education Accessible for Every Child üåü</strong><br>  <em>Powered by Gemma 3n ‚Ä¢ Built with ‚ù§Ô∏è for the Impact Challenge</em></p>